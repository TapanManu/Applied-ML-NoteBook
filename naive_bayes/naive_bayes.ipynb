{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjSMzoTHp8r+Gq4teH3w4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapanManu/Applied-ML-NoteBook/blob/master/naive_bayes/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aZsm1TOyznG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prior probability\n",
        "#p(y=class1),p(y=class2),pr(y=class3)\n",
        "\n",
        "#posterior probability\n",
        "#p(y=class1|x=class2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aClUTtUXN35A",
        "colab_type": "text"
      },
      "source": [
        "Bayes Rule\n",
        "\n",
        "posterior probability  =\n",
        "(prior probability x likelihood )/evidence\n",
        "\n",
        "p(y|x) = p(y) x p(x|y) / p(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eETdJmZiO8dO",
        "colab_type": "text"
      },
      "source": [
        "Naive assumption = the given class labels,features are assumed\n",
        "to be independent of each other\n",
        "\n",
        "The predicted label y is the y that maximizes, the argument that maximizes this computation of probability of y given X.\n",
        "\n",
        "Which is computed using Bayes Rule as probability of y, that is the prior, times T independent products of individual features given y.\n",
        "\n",
        "the true label, or the predicted label I should say, y* is the y that maximizes probabilit of y given X.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z91O6W_PTkq",
        "colab_type": "text"
      },
      "source": [
        "So for example, if the query is Python download, you're going to say \"The predicted y is the y that maximizes probability of y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkk20DYwP3Lq",
        "colab_type": "text"
      },
      "source": [
        "features of naive bayes\n",
        "1. The prior probabilities are these probabilities for each of the classes in your set of classes.\n",
        "\n",
        "2.likelihood:The probability of x_i given y that is import, no, that is needed for all features x_i and all labels y in Y.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FMgIhIQpzI",
        "colab_type": "text"
      },
      "source": [
        "You are training a naïve Bayes classifier, where the number of possible labels, |Y| = 3 and the dimension of the data element, |X| = 100, where every feature (dimension) is binary. How many parameters does the naïve Bayes classification model have?\n",
        "\n",
        "|Y| + 2 x |X| +x|Y| = 603\n",
        "Correct \n",
        "A naïve Bayes classifier has two kinds of parameters:\n",
        "\n",
        "    Pr(y) for every y in Y: so if |Y| = 3, there are three such parameters.\n",
        "    Pr(x_i | y) for every binary feature x_i in X and y in Y. Specifically, for a particular feature x_1, the parameters are Pr(x_1 = 1 | y) and Pr(x_1 = 0 | y) for every y. So if |X| = 100 binary features and |Y| = 3, there are (2 x 100) x 3 = 600 such features\n",
        "\n",
        "Hence in all, there are 603 features.\n",
        "Note that not all of these features areindependent. In particular, Pr(x_i = 0 | y) = 1 - Pr(x_i = 1 | y), for every x_i and y. So, there are only 300 independent parameters of this kind (as the other 300 parameters are just complements of these). Similarly, the sum of all prior probabilities Pr(y) should be 1. This means there are only 2 independent prior probabilities. In all, for this example, there are 302 independent parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX9PlR9JQ6vA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}